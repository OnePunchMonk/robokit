# ü§ñ RoboKit
A toolkit for robotic tasks

## üöÄ Projects Using RoboKit
Chronologically listed (latest first):
- Perception: [MRVG](https://irvlutd.github.io/MultiGrounding/)
- Mobile Manipulation: [HRT1](https://irvlutd.github.io/HRT1/)
- Interactive Robot Teaching: [iTeach](https://irvlutd.github.io/iTeach/)
- Robot Exploration and Navigation: [AutoX-SemMap](https://irvlutd.github.io/SemanticMapping/) 
- Perception Research: [NIDS-Net](https://irvlutd.github.io/NIDSNet)
- Grasp Trajectory Optimization: [GTO](https://irvlutd.github.io/GraspTrajOpt/)

## ‚ú® Features
- Docker Support
- **Docker Support**
  - Base image with ROS Noetic + CUDA 11.8 + Ubuntu 20.04 + Gazebo 11  
    ‚Üí [`Dockerfile`](docker/Dockerfile-ub20.04-ros-noetic-cuda11.8-gazebo)
  - Refer to BundleSDF's [Docker setup](https://github.com/NVlabs/BundleSDF?tab=readme-ov-file#dockerenvironment-setup)
  - Quickstart script: [`run_container.sh`](docker/run_container.sh)

- **Zero-Shot Capabilities**
  - üîç CLIP-based classification  
  - üéØ Text-to-BBox: GroundingDINO  
  - üßº BBox-to-Mask: Segment Anything (MobileSAM)  
  - üìè Image-to-Depth: Depth Anything  
  - üîº Feature Upsampling: FeatUp  
  - üö™ DoorHandle Detection: iTeach‚ÄìDHYOLO ([demo](https://huggingface.co/spaces/IRVLUTD/DH-YOLO))  
  - üìΩÔ∏è Mask Propagation for Videos: SegmentAnythingV2 (SAMv2)
    - Input: `jpg` or `mp4`
    - Supports:
      - Point/BBox prompts across video frames
      - Multi-object point collection
    - Tip: Use jpgs for frame-wise prediction; skip conversion for single images
    - Note that SAMv2 only supports mp4 or jpg files as of 11/06/2024
    - If you have an mp4 file then extract individual frames as jpg and store in a directory
    - For single image mask predictions, no need to convert to jpg.

## ‚öôÔ∏è Getting Started

### üß∞ Prerequisites
- Python 3.7 or higher (tested 3.9.18)
- torch (tested 2.0)
- torchvision
- pytorch-cuda=11.8 (tested)
- [SAMv2 requires py>=3.10.0](https://github.com/facebookresearch/sam2/blob/c2ec8e14a185632b0a5d8b161928ceb50197eddc/setup.py#L171) (here the installation has been tweaked to remove this constraint)

### üõ†Ô∏è Installation
```sh
# clone
git clone https://github.com/IRVLUTD/robokit.git && cd robokit 

# make sure your CUDA_HOME env var is set
export CUDA_HOME=/usr/local/cuda

# install dependencies
pip install -r requirements.txt
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# install
python setup.py install
```

üß© Known Installation Issues
- Check GroundingDINO [installation](https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file#hammer_and_wrench-install) for the following error
```sh
NameError: name '_C' is not defined
```
- For SAMv2, `ModuleNotFoundError: No module named 'omegaconf.vendor'`
```sh
pip install --upgrade --force-reinstall hydra-core
```

üß™ Usage
- Note: All test scripts are located in the [`test`](test) directory. Place the respective test scripts in the root directory to run.
- SAM: [`test_sam.py`](test/test_sam.py)
- GroundingDINO + SAM: [`test_gdino_sam.py`](test/test_gdino_sam.py)
- GroundingDINO + SAM + CLIP: [`test_gdino_sam_clip.py`](test/test_gdino_sam_clip.py)
- Depth Anything: [`test_depth_anything.py`](test/test_depth_anything.py)
- FeatUp: [`test_featup.py`](test/test_featup.py)
- iTeach-DHYOLO: [`test_dhyolo.py`](test/test_dhyolo.py)
- SAMv2: 
  - [`collect_point_prompts.py`](test/collect_point_prompts.py)
  - [`test_samv2_1_bbox_prompt.py`](test/test_samv2_1_bbox_prompt.py)
  - [`test_samv2_point_prompts.py`](test/test_samv2_point_prompts.py)
  - [`test_gdino_sam2_img.py`](test/test_gdino_sam2_img.py)
- Test Datasets: [`test_dataset.py`](test/test_dataset.py)
  - `python test_dataset.py --gpu 0 --dataset <ocid_object_test/osd_object_test>`

## üõ£Ô∏è Roadmap
Planned improvements:
- Config-based pretrained checkpoint switching
- ‚ú® More features coming soon...


## üôè Acknowledgments

This project is based on the following repositories (license check mandatory):
- [CLIP](https://github.com/openai/CLIP)
- [MobileSAM](https://github.com/ChaoningZhang/MobileSAM)
- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)
- [DepthAnything](https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation)
- [FeatUp](https://github.com/mhamilton723/FeatUp)
- [iTeach](https://irvlutd.github.io/iTeach/)-[DHYOLO](https://huggingface.co/spaces/IRVLUTD/DH-YOLO)
- [SAMv2](https://github.com/facebookresearch/sam2)


Special thanks to Dr. [Yu Xiang](https://yuxng.github.io/), [Sai Haneesh Allu](https://saihaneeshallu.github.io/), and [Itay Kadosh](https://scholar.google.com/citations?user=1ZLE5jsAAAAJ&hl=en) for their early feedback.

## üìú License
This project is licensed under the MIT License. However, before using this tool please check the respective works for specific licenses.
